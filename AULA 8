REDES NEURAIS:
Segundo a SBC (sociedade brasileira de computação), 70% dos artigos publicados são sobre redes neurais, ou seja, existe muita pesquisa em cima disso, usando classificações de imagens e dados;

Redes Neurais é muito vasto, por isso veremos pouca coisa.

Quantos neurônios o ser humano tem?
86 Bilhões;

Cada neurônio se conecta com outros neurônios, o modelo de Rede Neural é um modelo matemático, aonde eu calculo aonde a transmissão de neurônios acontece.
86 bilhões faz em média 7mil pulsos elétricos para o cérebro, ou seja, são muitos.

IA usa como base, Redes Neurais, o que se torna extremamente complexo.

A conexão de blocos básicos denominados "Neurônios Artificiais", aonde seu objetivo era simular a habilidade humana.
Vale ressaltar que os modelos matemáticos são pesados computacionalmente falando, necessitando de um processamento melhor e mais poderoso.

Todos os cálculos da rede neural era feito na mão até 2010, até o nascimento do python, machine learning e framework.

O que é um neurônio? 
É a ativação de um estímulo, que percorre desde o terminal do Axônio até o corpo, na qual, dependendo de sua força, pode ser ativado ou não.
Essa reação vai provocar alguma reação, piscar, mexer o dedo e etc.

E Neurônio artificial?
Este faz parte da Rede Neural, criada por um matemático e um psicólogo.
Quando mais um impulso é potencializado, mais neurônios eu ativo conforme meu programa avança.

O mais difícil é rodar e configurar os parâmetros, até conseguir rodar até o resto da vida.

PERCEPTRON:
O primeiro foi criado em 1958, por Franck Rosenblatt, psicólogo, mas a ideia em ci foi criada em 1943, por Mcculloch and Pitts, mas que não evoluiu.

A "fórmula" consiste em:
Várias entradas se somam, entram em uma outra função, até chegar a -1 até 1, dizendo se o neurônio ativa ou não.

w1 * x1 - w2 * x2: somatória da entrada * peso dela.
se > 0 = 1
se < 0 = 0

Toda Rede Neural tem entrada, processamento e saída.
Ex: o sistema recebe 3 entradas, significam 3 candidatos, aonde existem 2 analisadores;
se eu for analisar o candidato 1, os 2 analisadores devem analisar, ou seja;
Todos se comunicam com os analisadores, ou neurônios.

Após fazer a somatória (x1 * w1 + x2 * w2 + x3 * w3), entramos com a FUNÇÃO DE ATIVAÇÃO, a qual pode ser usada a RELU (mais voltada para CNNs);

ÉPOCAS:
Passagem completa por todo o treinamento, ou seja, ao treinar uma rede neural, precisaremos de uma rede de dados.
Durante uma época, todos os exemplos são apresentados à Rede Neural, com seus pesos ajustados com base nos erros das saídas esperadas.
Esta rede de dados vai definir o quão bom sua Rede Neural é.
Quanto mais dados, melhor. Vale ressaltar que não podemos usar os dados para teste, somente para treinamento.
Para equilibrar isso, fazemos 80/20, 80% treinamento 20% teste;

Épocas podem ser mais de uma, ou seja, 80 mil dados vezes 2, serão 2 épocas, esse número varia de acordo com o necessitado.
Muitas épocas podem causar uma Rede Neural que aprendeu muito e não sabe aprender novos dados.
Poucas podem causar uma rede que aprendeu pouco demais, mas sabe aprender novos.

Quando treinamos com 80%, precisamos testar os 20%;

FUNÇÕES DE ATIVAÇÃO:
Cada uma tem seu propósito, mas a mais usada é a ReLU, ela lida bem com as saídas que são abaixo de 0, ex:
  A saída foi -0,5 e a outra foi 0,78 o que resulta em -0,39
  a saída da ativação foi -0,31 , o que se tornará positivo fazendo -0 * -0, fazendo com que a ReLU não use todos simultaneamente, mas usará todos em algum momento.

Quando usar qual? 
Depende da característica do problema.

FUNÇÃO DEGRAU:
Responde com sim ou não, não se preocuparemos com outros parâmetros.

FUNÇÃO LINEAR:
Recebe a entrada e multiplica cada peso do neurônio, melhor que a degrau, pois irá retornar valores 0 até 1.

FUNÇÃO SIGMOIDE:
Os valores variam entre 0 e 1, é voltada para probabilidade, sua função é:
f(x) = 1/1 + e^-x;

FUNÇÃO SOFTMAX:
Quando o problema tem mais de uma classe, ela se deriva da SIGMOIDE, sua função é:
softmax(x) = e^x^i/M (down J) e^x^i

TANGENTE HIPERBÓLICA:
Esta vai de -1 até 1, tem as análises ampliadas, tendo o dobro de grau.

ReLU:
Anula valores negativos, ou seja, menor que 0 é 0, maior que 0 é 1, sua função é:
g(z) = max(0, z) que é a mesma coisa que: {0, x < 0}
                                          {0, x >= 0}


Busque pesquisar mais um pouco.
